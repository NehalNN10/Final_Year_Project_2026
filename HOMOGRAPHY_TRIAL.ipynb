{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python numpy ultralytics"
      ],
      "metadata": {
        "id": "vNygJJM3GJy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTWdBCDGF3VQ",
        "outputId": "5664b65f-5ed0-418a-98a4-b4820a450caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tracker_config.yaml has been created and saved to the current working directory.\n"
          ]
        }
      ],
      "source": [
        "# CODE CELL 1: Create the fixed tracker config file\n",
        "yaml_content = \"\"\"\n",
        "tracker_type: 'bytetrack'\n",
        "track_high_thresh: 0.3\n",
        "track_low_thresh: 0.1\n",
        "new_track_thresh: 0.7\n",
        "track_buffer: 100\n",
        "match_thresh: 0.75\n",
        "fuse_score: True\n",
        "\"\"\"\n",
        "\n",
        "# Write the file to the current working directory (/kaggle/working)\n",
        "with open('tracker_config.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"tracker_config.yaml has been created and saved to the current working directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture('shortened_lab.mp4')\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video file. Check the path to 'input_video.mp4'.\")"
      ],
      "metadata": {
        "id": "HvR-cnZXpaUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from ultralytics import YOLO # Library for YOLOv8 Object Detection\n",
        "\n",
        "# --- 1. HOMOGRAPHY SETUP (CALIBRATION) ---\n",
        "\n",
        "# Define the coordinates that link your video feed to your 12x6 map.\n",
        "# *YOU MUST REPLACE THESE WITH YOUR REAL-WORLD COORDINATES*\n",
        "VIDEO_PTS_SRC = np.array([\n",
        "    [10, 320],  # Corresponds to Top-Left in a flat map view\n",
        "    [840, 320], # Corresponds to Top-Right\n",
        "    [850, 479], # Corresponds to Bottom-Right\n",
        "    [0, 479]   # Bottom-Left\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Map dimensions (e.g., 600x300 pixels to represent your 12x6 area)\n",
        "MAP_WIDTH, MAP_HEIGHT = 850, 200\n",
        "MAP_PTS_DST = np.array([\n",
        "    [50, 20],   # P1: Top-Left in map (Y=20)\n",
        "    [800, 20],  # P2: Top-Right (Y=20)\n",
        "    [800, 180], # P3: Bottom-Right (Y=180)\n",
        "    [50, 180] # Corresponds to Bottom-Left\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Calculate Homography Matrix (H) once\n",
        "H, _ = cv2.findHomography(VIDEO_PTS_SRC, MAP_PTS_DST)\n",
        "\n",
        "# Map/Visualization Setup\n",
        "TRACE_POINTS = {} # Dictionary to store trace points for multiple people (ID: [points])\n",
        "MAP_IMAGE = np.zeros((MAP_HEIGHT, MAP_WIDTH, 3), dtype=np.uint8)\n",
        "\n",
        "# --- 2. YOLOv8 MODEL & VIDEO SETUP ---\n",
        "\n",
        "# Load the pre-trained YOLOv8s model (as used in the paper [cite: 16])\n",
        "model = YOLO('yolo12n.pt')\n",
        "\n",
        "# Replace 'input_video.mp4' with the path to your video file\n",
        "cap = cv2.VideoCapture('shortened_lab.mp4')\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open video file. Check the path to 'input_video.mp4'.\")\n",
        "    exit()\n",
        "\n",
        "# Set up VideoWriter to save the result\n",
        "FPS = cap.get(cv2.CAP_PROP_FPS) or 20\n",
        "VIDEO_FILENAME = 'yolo_mapped_path.avi'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter(VIDEO_FILENAME, fourcc, FPS, (MAP_WIDTH, MAP_HEIGHT))\n",
        "\n",
        "print(\"Starting YOLO tracking and mapping simulation (Saving to video)...\")\n",
        "\n",
        "# --- 3. MAIN PROCESSING LOOP ---\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # --- A. OBJECT DETECTION & TRACKING (YOLO) ---\n",
        "    # The 'persist=True' and 'tracker=\"bytetrack.yaml\"' enable multi-object tracking\n",
        "    # YOLO automatically assigns a unique 'track_id' to each detected person.\n",
        "    results = model.track(frame, persist=True, tracker=\"tracker_config.yaml\", classes=[0], verbose=False)\n",
        "\n",
        "    # Reset map image for the current frame\n",
        "    MAP_IMAGE = np.zeros((MAP_HEIGHT, MAP_WIDTH, 3), dtype=np.uint8)\n",
        "\n",
        "    # 1. Highlight the Mapped Area Boundary\n",
        "    cv2.polylines(MAP_IMAGE, [MAP_PTS_DST.astype(np.int32).reshape((-1, 1, 2))], isClosed=True, color=(255, 255, 0), thickness=2)\n",
        "\n",
        "    # Check if tracking data is available\n",
        "    if results[0].boxes.id is not None:\n",
        "        boxes = results[0].boxes.xywh.cpu().numpy()\n",
        "        track_ids = results[0].boxes.id.int().cpu().numpy()\n",
        "\n",
        "        for box, track_id in zip(boxes, track_ids):\n",
        "            # Bounding Box (x_center, y_center, w, h)\n",
        "            x_c, y_c, w, h = box\n",
        "            x1 = int(x_c - w/2)\n",
        "            y1 = int(y_c - h/2)\n",
        "            x2 = int(x_c + w/2)\n",
        "            y2 = int(y_c + h/2)\n",
        "\n",
        "            # TRUE feet position = midpoint of the bottom edge of the box\n",
        "            feet_x_cam = int((x1 + x2) / 2)\n",
        "            feet_y_cam = int(y2)\n",
        "\n",
        "            # --- B. HOMOGRAPHY TRANSFORMATION ---\n",
        "            camera_point = np.array([[feet_x_cam, feet_y_cam]], dtype=np.float32)\n",
        "            mapped_point = cv2.perspectiveTransform(np.array([camera_point]), H)\n",
        "\n",
        "            map_x, map_y = mapped_point[0][0].astype(int)\n",
        "\n",
        "            # 3. STORE AND DRAW TRACE\n",
        "            track_id_str = str(track_id)\n",
        "            if track_id_str not in TRACE_POINTS:\n",
        "                TRACE_POINTS[track_id_str] = []\n",
        "\n",
        "            # Add the current position to the trace history for this person\n",
        "            TRACE_POINTS[track_id_str].append((map_x, map_y))\n",
        "\n",
        "            # Draw the full path for this person\n",
        "            path = TRACE_POINTS[track_id_str]\n",
        "            if len(path) > 1:\n",
        "                # Use a different color for each person's path (based on ID)\n",
        "                trail_color_code = int(track_id) * 50 % 255\n",
        "                trail_color = (trail_color_code, 255 - trail_color_code, 100)\n",
        "\n",
        "                points_array = np.array(path, np.int32)\n",
        "                cv2.polylines(MAP_IMAGE, [points_array.reshape((-1, 1, 2))], isClosed=False, color=trail_color, thickness=2)\n",
        "\n",
        "            # 4. DRAW CURRENT POSITION (Dot) on the map\n",
        "            cv2.circle(MAP_IMAGE, (map_x, map_y), radius=6, color=(0, 0, 255), thickness=-1)\n",
        "            cv2.putText(MAP_IMAGE, str(track_id), (map_x + 8, map_y - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "            # Optional: Draw on the source video frame for verification\n",
        "            cv2.circle(frame, (feet_x_cam, feet_y_cam), 5, (0, 255, 255), -1)\n",
        "            cv2.putText(frame, f\"ID:{track_id}\", (feet_x_cam, feet_y_cam - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
        "\n",
        "    # --- C. WRITE AND DISPLAY ---\n",
        "    video_writer.write(MAP_IMAGE)\n",
        "\n",
        "    cv2_imshow(MAP_IMAGE)\n",
        "    cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "UprB7K01GCM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0x25_PPQa4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}